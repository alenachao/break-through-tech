{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 4: Optimizing Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, you will continue practicing the fourth step of the machine learning life cycle and train logistic regression models that will be used to solve a classification problem.  You will build many variants, each one with a different value of the $C$ hyperparameter, which governs the amount of regularization used. Regularization is a process where we add a \"penalty\" to the original log loss function. This penalty is a function of the magnitudes of the weights learned in the Logistic Regression. The following shows the regularized log loss using what is called \"L2\" regularization.<br><br> \n",
    "\n",
    "<center>$Regularized \\ LogLoss = -\\frac{1}{N} \\sum\\limits_{i=1}^N (y_ilog(P_i)+(1-y_i)log(1-P_i))+\\frac{1}{C} \\sum\\limits_{j=1}^m w_j^2$</center><br><br>\n",
    "\n",
    "\n",
    "With L2 regularization, the penalty is the sum of the squares of the weights scaled by a constant $1/C$. When the hyperparameter $C$ is large, we reduce the weight of the penalty, which results in less regularization. You will build Logistic regressions with different values of $C$ and will check how this impacts the log loss.\n",
    "\n",
    "\n",
    "You will complete the following tasks:\n",
    "\n",
    "1. Build your DataFrame and define your ML problem:\n",
    "    * Load the \"cell2cell\" data set into a DataFrame\n",
    "    * Define the label - what are you predicting?\n",
    "    * Identify features\n",
    "3. Create labeled examples from the data set\n",
    "4. Split the data into training and test data sets\n",
    "5. Train logistic regression classifiers and evaluate their performances:\n",
    "    * Fit logistic regression models to the training data using different hyperparameter values per classifier\n",
    "    * Evaluate the accuracy of each model's predictions\n",
    "    * Plot and analyize the resulting log loss and accuracy scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1. Build Your DataFrame and Define Your ML Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load a Data Set and Save it as a Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will work with the \"cell2celltrain\" data set. This version of the data set has been preprocessed and is ready for modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do not remove or edit the line below:\n",
    "filename = os.path.join(os.getcwd(), \"data\", \"cell2celltrain.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Load the data and save it to DataFrame `df`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Churn</th>\n",
       "      <th>ServiceArea</th>\n",
       "      <th>ChildrenInHH</th>\n",
       "      <th>HandsetRefurbished</th>\n",
       "      <th>HandsetWebCapable</th>\n",
       "      <th>TruckOwner</th>\n",
       "      <th>RVOwner</th>\n",
       "      <th>HomeownershipKnown</th>\n",
       "      <th>BuysViaMailOrder</th>\n",
       "      <th>...</th>\n",
       "      <th>HandsetModels</th>\n",
       "      <th>CurrentEquipmentDays</th>\n",
       "      <th>AgeHH1</th>\n",
       "      <th>AgeHH2</th>\n",
       "      <th>RetentionCalls</th>\n",
       "      <th>RetentionOffersAccepted</th>\n",
       "      <th>ReferralsMadeBySubscriber</th>\n",
       "      <th>IncomeGroup</th>\n",
       "      <th>AdjustmentsToCreditRating</th>\n",
       "      <th>HandsetPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3000002</td>\n",
       "      <td>True</td>\n",
       "      <td>SEAPOR503</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>-0.077013</td>\n",
       "      <td>1.387766</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>4.662897</td>\n",
       "      <td>-0.1283</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>-0.103411</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.864858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3000010</td>\n",
       "      <td>True</td>\n",
       "      <td>PITHOM412</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616775</td>\n",
       "      <td>3.019920</td>\n",
       "      <td>0.392039</td>\n",
       "      <td>0.871495</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.1283</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.864858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3000014</td>\n",
       "      <td>False</td>\n",
       "      <td>MILMIL414</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616775</td>\n",
       "      <td>3.019920</td>\n",
       "      <td>-0.241605</td>\n",
       "      <td>0.202910</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.1283</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.368174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3000022</td>\n",
       "      <td>False</td>\n",
       "      <td>PITHOM412</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694763</td>\n",
       "      <td>0.305179</td>\n",
       "      <td>-0.060564</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.1283</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-1.195980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3000026</td>\n",
       "      <td>True</td>\n",
       "      <td>OKCTUL918</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>...</td>\n",
       "      <td>1.590917</td>\n",
       "      <td>1.857585</td>\n",
       "      <td>0.663601</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.1283</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>1.489856</td>\n",
       "      <td>2.469282</td>\n",
       "      <td>-1.195980</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CustomerID  Churn ServiceArea  ChildrenInHH  HandsetRefurbished  \\\n",
       "0     3000002   True   SEAPOR503         False               False   \n",
       "1     3000010   True   PITHOM412          True               False   \n",
       "2     3000014  False   MILMIL414          True               False   \n",
       "3     3000022  False   PITHOM412         False               False   \n",
       "4     3000026   True   OKCTUL918         False               False   \n",
       "\n",
       "   HandsetWebCapable  TruckOwner  RVOwner  HomeownershipKnown  \\\n",
       "0               True       False    False                True   \n",
       "1              False       False    False                True   \n",
       "2              False       False    False               False   \n",
       "3               True       False    False                True   \n",
       "4              False       False    False                True   \n",
       "\n",
       "   BuysViaMailOrder  ...  HandsetModels  CurrentEquipmentDays    AgeHH1  \\\n",
       "0              True  ...       0.487071             -0.077013  1.387766   \n",
       "1              True  ...      -0.616775              3.019920  0.392039   \n",
       "2             False  ...      -0.616775              3.019920 -0.241605   \n",
       "3              True  ...       2.694763              0.305179 -0.060564   \n",
       "4              True  ...       1.590917              1.857585  0.663601   \n",
       "\n",
       "     AgeHH2  RetentionCalls  RetentionOffersAccepted  \\\n",
       "0 -0.883541        4.662897                  -0.1283   \n",
       "1  0.871495       -0.180167                  -0.1283   \n",
       "2  0.202910       -0.180167                  -0.1283   \n",
       "3 -0.883541       -0.180167                  -0.1283   \n",
       "4  1.372934       -0.180167                  -0.1283   \n",
       "\n",
       "   ReferralsMadeBySubscriber  IncomeGroup  AdjustmentsToCreditRating  \\\n",
       "0                  -0.169283    -0.103411                  -0.140707   \n",
       "1                  -0.169283     0.215243                  -0.140707   \n",
       "2                  -0.169283     0.533896                  -0.140707   \n",
       "3                  -0.169283     0.533896                  -0.140707   \n",
       "4                  -0.169283     1.489856                   2.469282   \n",
       "\n",
       "  HandsetPrice  \n",
       "0    -0.864858  \n",
       "1    -0.864858  \n",
       "2    -0.368174  \n",
       "3    -1.195980  \n",
       "4    -1.195980  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "df = pd.read_csv(filename)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define the Label\n",
    "\n",
    "This is a binary classification problem in which we will predict customer churn. The label is the `Churn` column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Identify Features\n",
    "\n",
    "To implement a Logistic Regression model, we must use only the numeric columns. \n",
    "\n",
    "\n",
    "<b>Task</b>: Use the Pandas DataFrame <code>select_dtypes()</code> method to obtain all of names of columns that have a dtype of \"float64.\" Save the result to a list named `feature_list`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MonthlyRevenue',\n",
       " 'MonthlyMinutes',\n",
       " 'TotalRecurringCharge',\n",
       " 'DirectorAssistedCalls',\n",
       " 'OverageMinutes',\n",
       " 'RoamingCalls',\n",
       " 'PercChangeMinutes',\n",
       " 'PercChangeRevenues',\n",
       " 'DroppedCalls',\n",
       " 'BlockedCalls',\n",
       " 'UnansweredCalls',\n",
       " 'CustomerCareCalls',\n",
       " 'ThreewayCalls',\n",
       " 'ReceivedCalls',\n",
       " 'OutboundCalls',\n",
       " 'InboundCalls',\n",
       " 'PeakCallsInOut',\n",
       " 'OffPeakCallsInOut',\n",
       " 'DroppedBlockedCalls',\n",
       " 'CallForwardingCalls',\n",
       " 'CallWaitingCalls',\n",
       " 'MonthsInService',\n",
       " 'UniqueSubs',\n",
       " 'ActiveSubs',\n",
       " 'Handsets',\n",
       " 'HandsetModels',\n",
       " 'CurrentEquipmentDays',\n",
       " 'AgeHH1',\n",
       " 'AgeHH2',\n",
       " 'RetentionCalls',\n",
       " 'RetentionOffersAccepted',\n",
       " 'ReferralsMadeBySubscriber',\n",
       " 'IncomeGroup',\n",
       " 'AdjustmentsToCreditRating',\n",
       " 'HandsetPrice']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "feature_list = list(df.select_dtypes(include='float64').columns)\n",
    "feature_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2. Create Labeled Examples from the Data Set "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our data is fully prepared for modeling. We can now create labeled examples from DataFrame `df`.\n",
    "\n",
    "<b>Task</b>: Obtain the feature columns from DataFrame `df` and assign to `X`. Obtain the label column from DataFrame `df` and assign to `y`.\n",
    "\n",
    "You should have 51047 labeled examples. Each example contains 35 features and one label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MonthlyRevenue</th>\n",
       "      <th>MonthlyMinutes</th>\n",
       "      <th>TotalRecurringCharge</th>\n",
       "      <th>DirectorAssistedCalls</th>\n",
       "      <th>OverageMinutes</th>\n",
       "      <th>RoamingCalls</th>\n",
       "      <th>PercChangeMinutes</th>\n",
       "      <th>PercChangeRevenues</th>\n",
       "      <th>DroppedCalls</th>\n",
       "      <th>BlockedCalls</th>\n",
       "      <th>...</th>\n",
       "      <th>HandsetModels</th>\n",
       "      <th>CurrentEquipmentDays</th>\n",
       "      <th>AgeHH1</th>\n",
       "      <th>AgeHH2</th>\n",
       "      <th>RetentionCalls</th>\n",
       "      <th>RetentionOffersAccepted</th>\n",
       "      <th>ReferralsMadeBySubscriber</th>\n",
       "      <th>IncomeGroup</th>\n",
       "      <th>AdjustmentsToCreditRating</th>\n",
       "      <th>HandsetPrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.782676</td>\n",
       "      <td>-0.578738</td>\n",
       "      <td>-1.041153</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>-0.414422</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>-0.564836</td>\n",
       "      <td>-0.449987</td>\n",
       "      <td>-0.587303</td>\n",
       "      <td>-0.309284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>-0.077013</td>\n",
       "      <td>1.387766</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>4.662897</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>-0.103411</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.864858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.940180</td>\n",
       "      <td>-0.973177</td>\n",
       "      <td>-1.250809</td>\n",
       "      <td>-0.401714</td>\n",
       "      <td>-0.414422</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.029311</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>-0.631532</td>\n",
       "      <td>-0.373230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616775</td>\n",
       "      <td>3.019920</td>\n",
       "      <td>0.392039</td>\n",
       "      <td>0.871495</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.215243</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.864858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.468118</td>\n",
       "      <td>-0.976952</td>\n",
       "      <td>-0.370255</td>\n",
       "      <td>-0.401714</td>\n",
       "      <td>-0.414422</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.037077</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>-0.664703</td>\n",
       "      <td>-0.373230</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.616775</td>\n",
       "      <td>3.019920</td>\n",
       "      <td>-0.241605</td>\n",
       "      <td>0.202910</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.368174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.526784</td>\n",
       "      <td>1.484048</td>\n",
       "      <td>1.181196</td>\n",
       "      <td>0.154708</td>\n",
       "      <td>-0.414422</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.654524</td>\n",
       "      <td>0.234797</td>\n",
       "      <td>4.012499</td>\n",
       "      <td>0.330172</td>\n",
       "      <td>...</td>\n",
       "      <td>2.694763</td>\n",
       "      <td>0.305179</td>\n",
       "      <td>-0.060564</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-1.195980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.936810</td>\n",
       "      <td>-0.992050</td>\n",
       "      <td>-1.250809</td>\n",
       "      <td>-0.401714</td>\n",
       "      <td>-0.414422</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.044844</td>\n",
       "      <td>0.025066</td>\n",
       "      <td>-0.664703</td>\n",
       "      <td>-0.373230</td>\n",
       "      <td>...</td>\n",
       "      <td>1.590917</td>\n",
       "      <td>1.857585</td>\n",
       "      <td>0.663601</td>\n",
       "      <td>1.372934</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>1.489856</td>\n",
       "      <td>2.469282</td>\n",
       "      <td>-1.195980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51042</th>\n",
       "      <td>-0.233099</td>\n",
       "      <td>-0.301309</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>-0.383361</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>0.363618</td>\n",
       "      <td>-0.126582</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>0.573107</td>\n",
       "      <td>1.659328</td>\n",
       "      <td>1.790800</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.533896</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.368174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51043</th>\n",
       "      <td>0.816402</td>\n",
       "      <td>2.301236</td>\n",
       "      <td>1.600507</td>\n",
       "      <td>0.042526</td>\n",
       "      <td>0.051479</td>\n",
       "      <td>0.352789</td>\n",
       "      <td>0.518608</td>\n",
       "      <td>0.431894</td>\n",
       "      <td>1.181852</td>\n",
       "      <td>-0.309284</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>0.328819</td>\n",
       "      <td>0.754122</td>\n",
       "      <td>1.122214</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>1.489856</td>\n",
       "      <td>2.469282</td>\n",
       "      <td>-0.368174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51044</th>\n",
       "      <td>-0.233099</td>\n",
       "      <td>-0.301309</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>-0.383361</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>4.012499</td>\n",
       "      <td>0.019579</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>-0.010031</td>\n",
       "      <td>0.210998</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>0.852549</td>\n",
       "      <td>2.469282</td>\n",
       "      <td>-0.037051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51045</th>\n",
       "      <td>-0.233099</td>\n",
       "      <td>-0.301309</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>-0.383361</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>-0.664703</td>\n",
       "      <td>-0.373230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.487071</td>\n",
       "      <td>0.206676</td>\n",
       "      <td>0.029957</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>-0.180167</td>\n",
       "      <td>-0.128300</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>1.489856</td>\n",
       "      <td>-0.140707</td>\n",
       "      <td>-0.864858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51046</th>\n",
       "      <td>-0.233099</td>\n",
       "      <td>-0.301309</td>\n",
       "      <td>-0.076738</td>\n",
       "      <td>-0.289532</td>\n",
       "      <td>-0.383361</td>\n",
       "      <td>-0.125914</td>\n",
       "      <td>0.025428</td>\n",
       "      <td>0.022539</td>\n",
       "      <td>1.402996</td>\n",
       "      <td>-0.309284</td>\n",
       "      <td>...</td>\n",
       "      <td>3.798609</td>\n",
       "      <td>-1.203887</td>\n",
       "      <td>-1.418373</td>\n",
       "      <td>-0.883541</td>\n",
       "      <td>4.662897</td>\n",
       "      <td>6.891363</td>\n",
       "      <td>-0.169283</td>\n",
       "      <td>-1.378025</td>\n",
       "      <td>2.469282</td>\n",
       "      <td>-0.368174</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>51047 rows × 35 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       MonthlyRevenue  MonthlyMinutes  TotalRecurringCharge  \\\n",
       "0           -0.782676       -0.578738             -1.041153   \n",
       "1           -0.940180       -0.973177             -1.250809   \n",
       "2           -0.468118       -0.976952             -0.370255   \n",
       "3            0.526784        1.484048              1.181196   \n",
       "4           -0.936810       -0.992050             -1.250809   \n",
       "...               ...             ...                   ...   \n",
       "51042       -0.233099       -0.301309             -0.076738   \n",
       "51043        0.816402        2.301236              1.600507   \n",
       "51044       -0.233099       -0.301309             -0.076738   \n",
       "51045       -0.233099       -0.301309             -0.076738   \n",
       "51046       -0.233099       -0.301309             -0.076738   \n",
       "\n",
       "       DirectorAssistedCalls  OverageMinutes  RoamingCalls  PercChangeMinutes  \\\n",
       "0                  -0.289532       -0.414422     -0.125914          -0.564836   \n",
       "1                  -0.401714       -0.414422     -0.125914           0.029311   \n",
       "2                  -0.401714       -0.414422     -0.125914           0.037077   \n",
       "3                   0.154708       -0.414422     -0.125914           0.654524   \n",
       "4                  -0.401714       -0.414422     -0.125914           0.044844   \n",
       "...                      ...             ...           ...                ...   \n",
       "51042              -0.289532       -0.383361     -0.125914           0.025428   \n",
       "51043               0.042526        0.051479      0.352789           0.518608   \n",
       "51044              -0.289532       -0.383361     -0.125914           0.025428   \n",
       "51045              -0.289532       -0.383361     -0.125914           0.025428   \n",
       "51046              -0.289532       -0.383361     -0.125914           0.025428   \n",
       "\n",
       "       PercChangeRevenues  DroppedCalls  BlockedCalls  ...  HandsetModels  \\\n",
       "0               -0.449987     -0.587303     -0.309284  ...       0.487071   \n",
       "1                0.030120     -0.631532     -0.373230  ...      -0.616775   \n",
       "2                0.030120     -0.664703     -0.373230  ...      -0.616775   \n",
       "3                0.234797      4.012499      0.330172  ...       2.694763   \n",
       "4                0.025066     -0.664703     -0.373230  ...       1.590917   \n",
       "...                   ...           ...           ...  ...            ...   \n",
       "51042            0.022539      0.363618     -0.126582  ...       0.487071   \n",
       "51043            0.431894      1.181852     -0.309284  ...       0.487071   \n",
       "51044            0.022539      4.012499      0.019579  ...       0.487071   \n",
       "51045            0.022539     -0.664703     -0.373230  ...       0.487071   \n",
       "51046            0.022539      1.402996     -0.309284  ...       3.798609   \n",
       "\n",
       "       CurrentEquipmentDays    AgeHH1    AgeHH2  RetentionCalls  \\\n",
       "0                 -0.077013  1.387766 -0.883541        4.662897   \n",
       "1                  3.019920  0.392039  0.871495       -0.180167   \n",
       "2                  3.019920 -0.241605  0.202910       -0.180167   \n",
       "3                  0.305179 -0.060564 -0.883541       -0.180167   \n",
       "4                  1.857585  0.663601  1.372934       -0.180167   \n",
       "...                     ...       ...       ...             ...   \n",
       "51042              0.573107  1.659328  1.790800       -0.180167   \n",
       "51043              0.328819  0.754122  1.122214       -0.180167   \n",
       "51044             -0.010031  0.210998 -0.883541       -0.180167   \n",
       "51045              0.206676  0.029957 -0.883541       -0.180167   \n",
       "51046             -1.203887 -1.418373 -0.883541        4.662897   \n",
       "\n",
       "       RetentionOffersAccepted  ReferralsMadeBySubscriber  IncomeGroup  \\\n",
       "0                    -0.128300                  -0.169283    -0.103411   \n",
       "1                    -0.128300                  -0.169283     0.215243   \n",
       "2                    -0.128300                  -0.169283     0.533896   \n",
       "3                    -0.128300                  -0.169283     0.533896   \n",
       "4                    -0.128300                  -0.169283     1.489856   \n",
       "...                        ...                        ...          ...   \n",
       "51042                -0.128300                  -0.169283     0.533896   \n",
       "51043                -0.128300                  -0.169283     1.489856   \n",
       "51044                -0.128300                  -0.169283     0.852549   \n",
       "51045                -0.128300                  -0.169283     1.489856   \n",
       "51046                 6.891363                  -0.169283    -1.378025   \n",
       "\n",
       "       AdjustmentsToCreditRating  HandsetPrice  \n",
       "0                      -0.140707     -0.864858  \n",
       "1                      -0.140707     -0.864858  \n",
       "2                      -0.140707     -0.368174  \n",
       "3                      -0.140707     -1.195980  \n",
       "4                       2.469282     -1.195980  \n",
       "...                          ...           ...  \n",
       "51042                  -0.140707     -0.368174  \n",
       "51043                   2.469282     -0.368174  \n",
       "51044                   2.469282     -0.037051  \n",
       "51045                  -0.140707     -0.864858  \n",
       "51046                   2.469282     -0.368174  \n",
       "\n",
       "[51047 rows x 35 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "0         True\n",
       "1         True\n",
       "2        False\n",
       "3        False\n",
       "4         True\n",
       "         ...  \n",
       "51042     True\n",
       "51043    False\n",
       "51044     True\n",
       "51045    False\n",
       "51046    False\n",
       "Name: Churn, Length: 51047, dtype: bool"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "X = df[feature_list]\n",
    "display(X)\n",
    "y = df['Churn']\n",
    "display(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3. Create Training and Test Data Sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Create training and test data sets out of the labeled examples. Save the results to variables `X_train, X_test, y_train, y_test`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=1234)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: Check the dimensions of the training and test data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(34201, 35) (16846, 35) (34201,) (16846,)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4. Train a Logistic Regression Classifier and Evaluate the Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code cell below contains a function definition `train_test_LR()`. \n",
    "\n",
    "Inspect the function definition `train_test_LR(X_train, X_test, y_train, y_test, c=1)`. The function expects the training and test data sets, as well as a value for hyperparameter $C$. Note that we supplied the value of 1 for $C$ by default.\n",
    "\n",
    "<b>Task:</b> Complete the function to make it work.\n",
    "\n",
    "This function should:\n",
    "1. train a Logistic Regression model on the training data\n",
    "2. test the resulting model on the test data\n",
    "3. compute and return two items:\n",
    "    * the log loss of the resulting probability predictions on the test data \n",
    "    * the accuracy score of the resulting predicted class labels on the test data\n",
    "\n",
    "\n",
    "You will use the scikit-learn [```LogisticRegression``` class](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) and will provide the arguments `C=c` when creating the model object.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_LR(X_train, y_train, X_test, y_test, c=1):\n",
    "    '''\n",
    "    Fit a Linear Regression classifier to the training data X_train, y_train.\n",
    "    Return the loss and accuracy of resulting predictions on the test set.\n",
    "    Parameters:\n",
    "        C = Factor that controls how much regularization is applied to the model.\n",
    "    '''\n",
    "    \n",
    "    # 1. Create the LogisticRegression model object below and assign to variable 'model'\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    model = LogisticRegression(C=c)\n",
    "\n",
    "    # 2. Fit the model to the training data below\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    # 3. Make predictions on the test data using the predict_proba() method and assign the \n",
    "    # result to the variable 'probability_predictions' below\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    probability_predictions = model.predict_proba(X_test)\n",
    "\n",
    "    # print the first 5 probability class predictions\n",
    "    df_print = pd.DataFrame(probability_predictions, columns = ['Class: False', 'Class: True'])\n",
    "#     print('Class Prediction Probabilities: \\n' + df_print[0:5].to_string(index=False))\n",
    "\n",
    "    # 4. Compute the log loss on 'probability_predictions' and save the result to the variable\n",
    "    # 'l_loss' below\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    l_loss = log_loss(y_test, probability_predictions)\n",
    "#     print('Log loss: ' + str(l_loss))\n",
    "\n",
    "\n",
    "    # 5. Make predictions on the test data using the predict() method and assign the result \n",
    "    # to the variable 'class_label_predictions' below\n",
    "\n",
    "    # YOUR CODE HERE\n",
    "    class_label_predictions = model.predict(X_test)\n",
    "\n",
    "    # print the first 5 class label predictions \n",
    "#     print('Class labels: ' + str(class_label_predictions[0:5]))\n",
    "\n",
    "    # 6.Compute the accuracy score on 'class_label_predictions' and save the result \n",
    "    # to the variable 'acc_score' below\n",
    "\n",
    "    acc_score = accuracy_score(y_test, class_label_predictions)\n",
    "#     print('Accuracy: ' + str(acc_score))\n",
    "    \n",
    "    return l_loss, acc_score\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train a Model and Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task:</b> Use your function `train_test_LR()` to train one Logistic Regression classifier with the default value of hyperparameter C (`c=1`). Print the resulting log loss and accuracy score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.5878612157234173, accuracy: 0.7097827377418972\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "loss, acc = train_test_LR(X_train, y_train, X_test, y_test, c=1)\n",
    "print('loss: ' + str(loss) + ', accuracy: ' + str(acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5. Train on Different Hyperparameter Values and Analyze the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will adjust the $C$ regularization hyperparameter to check its impact on the model's log loss and accuracy. Hyperparameter `C` stands for the inverse of regularization strength. Smaller values specify stronger regularization and a simpler model. Larger values specify weaker regularization and a more complex model.<br>\n",
    "\n",
    "The code cell below creates a list `cs` of twenty values of $C$.  Every item in the list has a value $10^i$ for every integer $i$ in the output of `range(-10,10)`. Run the code cell below and inspect the different values of $C$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1e-10,\n",
       " 1e-09,\n",
       " 1e-08,\n",
       " 1e-07,\n",
       " 1e-06,\n",
       " 1e-05,\n",
       " 0.0001,\n",
       " 0.001,\n",
       " 0.01,\n",
       " 0.1,\n",
       " 1,\n",
       " 10,\n",
       " 100,\n",
       " 1000,\n",
       " 10000,\n",
       " 100000,\n",
       " 1000000,\n",
       " 10000000,\n",
       " 100000000,\n",
       " 1000000000]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cs = [10**i for i in range(-10,10)]\n",
    "cs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Task</b>: In the code cell below, loop over list `cs` and train and evaluate a different Logistic Regression model for every value of $C$. Use your function `train_test_LR()`. Print the resulting log loss and accuracy scores per model.\n",
    "\n",
    "We will want to create visualizations that plot the resulting log loss and accuracy score for every value of hyperparameter $C$. Considering this, save the resulting log loss values and accuracy scores that your function returns to two different lists. You will use these lists to create plots later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c: 1e-10, loss: 0.6019882218839937, accuracy: 0.710198266650837\n",
      "c: 1e-09, loss: 0.6019879879688643, accuracy: 0.710198266650837\n",
      "c: 1e-08, loss: 0.6019856457586286, accuracy: 0.710198266650837\n",
      "c: 1e-07, loss: 0.6019623116656803, accuracy: 0.710198266650837\n",
      "c: 1e-06, loss: 0.6017368944992653, accuracy: 0.710198266650837\n",
      "c: 1e-05, loss: 0.6000102566181061, accuracy: 0.710198266650837\n",
      "c: 0.0001, loss: 0.5939550491932645, accuracy: 0.710198266650837\n",
      "c: 0.001, loss: 0.5882530046237049, accuracy: 0.7104950730143654\n",
      "c: 0.01, loss: 0.5876588226394373, accuracy: 0.7099014602873086\n",
      "c: 0.1, loss: 0.587835892808505, accuracy: 0.7099014602873086\n",
      "c: 1, loss: 0.5878612157234173, accuracy: 0.7097827377418972\n",
      "c: 10, loss: 0.5878648343540094, accuracy: 0.7098420990146028\n",
      "c: 100, loss: 0.5878651012583729, accuracy: 0.7098420990146028\n",
      "c: 1000, loss: 0.5878651279496574, accuracy: 0.7098420990146028\n",
      "c: 10000, loss: 0.5878651306188116, accuracy: 0.7098420990146028\n",
      "c: 100000, loss: 0.587865130885716, accuracy: 0.7098420990146028\n",
      "c: 1000000, loss: 0.5878651309124132, accuracy: 0.7098420990146028\n",
      "c: 10000000, loss: 0.5878651309150807, accuracy: 0.7098420990146028\n",
      "c: 100000000, loss: 0.587865130915354, accuracy: 0.7098420990146028\n",
      "c: 1000000000, loss: 0.5878651309153761, accuracy: 0.7098420990146028\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "losses = []\n",
    "accuracies = []\n",
    "for i in cs:\n",
    "    loss, acc = train_test_LR(X_train, y_train, X_test, y_test, c=i)\n",
    "    print('c: ' + str(i) + ', loss: ' + str(loss) + ', accuracy: ' + str(acc))\n",
    "    losses.append(loss)\n",
    "    accuracies.append(acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's visualize the results. \n",
    "\n",
    "Before we create plots, let's reformat the hyperparameter values in list `cs` so that they can be easily visualized in our plots. We will take the log 10 of the hyperparameter values and save it to a new list called `cs_log10`. Let's take a look at the original values and transformed values:\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1e-10, 1e-09, 1e-08, 1e-07, 1e-06, 1e-05, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000, 100000, 1000000, 10000000, 100000000, 1000000000]\n",
      "[-10.  -9.  -8.  -7.  -6.  -5.  -4.  -3.  -2.  -1.   0.   1.   2.   3.\n",
      "   4.   5.   6.   7.   8.   9.]\n"
     ]
    }
   ],
   "source": [
    "cs_log10 = np.log10(cs)\n",
    "\n",
    "print(cs)\n",
    "print(cs_log10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Plot Log Loss\n",
    "\n",
    "<b>Task:</b> Create a `seaborn` lineplot to plot the resulting log loss for every value of hyperparameter $C$. The hyperparameter $C$ should be plotted on the x axis and the log loss should be plotted on the y axis. Label the x and y axes accordingly. Use the transformed values of hyperparameter $C$ contained in the list `cs_log10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "ax = sns.lineplot(x=cs_log10, y=losses);\n",
    "ax.set_xlabel('regularization parameter C');\n",
    "ax.set_ylabel('log loss');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>: Which value of $C$ yields the best results, in terms of loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best value of C according to the plot is 10^-2.6 (since the plot shows log(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot Accuracy\n",
    "\n",
    "<b>Task:</b> Create a `seaborn` lineplot to plot the resulting accuracy score for every value of hyperparameter $C$. The hyperparameter $C$ should be plotted on the x axis and the accuracy score should be plotted on the y axis. Label the x and y axes accordingly. Use the transformed values of hyperparameter $C$ contained in the list `cs_log10`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "ax = sns.lineplot(x=cs_log10, y=accuracies);\n",
    "ax.set_xlabel('regularization parameter C');\n",
    "ax.set_ylabel('accuracy');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Analysis</b>: Which value of $C$ yields the best results, in terms of accuracy?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to the previous plot, this plot suggests 10^-2.6 seems to be the best choice for C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
